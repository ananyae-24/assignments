{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of planar_try.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Shresth grover"
      ],
      "metadata": {
        "id": "QY2peNdFEoGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iMXiHlSDpfar"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import random \n",
        "import numpy as np  \n",
        "from torch import Tensor\n",
        "from typing import Callable\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def U_1(z):\n",
        "  u = 0.5 * ((torch.norm(z, 2, dim=1) - 2) / 0.4) ** 2\n",
        "  u = u - torch.log(\n",
        "    torch.exp(-0.5 * ((z[:, 0] - 2) / 0.6) ** 2)\n",
        "    + torch.exp(-0.5 * ((z[:, 0] + 2) / 0.6) ** 2)\n",
        "                )\n",
        "  return u\n",
        "\n"
      ],
      "metadata": {
        "id": "CXyIDaIxZ6ND"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import MultivariateNormal\n",
        "class VariationalLoss(nn.Module):\n",
        "    def __init__(self, distribution):\n",
        "        super().__init__()\n",
        "        self.distr = distribution\n",
        "        self.base_distr = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
        "\n",
        "    def forward(self, z0: Tensor, z: Tensor, sum_log_det_J: float) -> float:\n",
        "        base_log_prob = self.base_distr.log_prob(z0)\n",
        "        target_density_log_prob = -self.distr(z)\n",
        "        return (base_log_prob - target_density_log_prob - sum_log_det_J).mean()\n",
        "     \n",
        "\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "HmAsyWRZpnLh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlanarTransform(nn.Module):\n",
        "\n",
        "    def __init__(self, dim: int = 2):\n",
        "        super().__init__()\n",
        "        self.w = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n",
        "        self.b = nn.Parameter(torch.randn(1).normal_(0, 0.1))\n",
        "        self.u = nn.Parameter(torch.randn(1, dim).normal_(0, 0.1))\n",
        "\n",
        "    def forward(self, z: Tensor) -> Tensor:\n",
        "        if torch.mm(self.u, self.w.T) < -1:\n",
        "            self.get_u_hat()\n",
        "\n",
        "        return z + self.u * nn.Tanh()(torch.mm(z, self.w.T) + self.b)\n",
        "\n",
        "    def log_det_J(self, z: Tensor) -> Tensor:\n",
        "        if torch.mm(self.u, self.w.T) < -1:\n",
        "            self.get_u_hat()\n",
        "        a = torch.mm(z, self.w.T) + self.b\n",
        "        psi = (1 - nn.Tanh()(a) ** 2) * self.w\n",
        "        abs_det = (1 + torch.mm(self.u, psi.T)).abs()\n",
        "        log_det = torch.log(1e-4 + abs_det)\n",
        "\n",
        "        return log_det\n",
        "\n",
        "    def get_u_hat(self) -> None:\n",
        "        wtu = torch.mm(self.u, self.w.T)\n",
        "        m_wtu = -1 + torch.log(1 + torch.exp(wtu))\n",
        "        self.u.data = (\n",
        "            self.u + (m_wtu - wtu) * self.w / torch.norm(self.w, p=2, dim=1) ** 2\n",
        "        )"
      ],
      "metadata": {
        "id": "38m7I0WfpsLq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "class PlanarFlow(nn.Module):\n",
        "    def __init__(self, dim: int = 2, K: int = 6):\n",
        "        super().__init__()\n",
        "        self.layers = [PlanarTransform(dim) for _ in range(K)]\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, z: Tensor) -> Tuple[Tensor, float]:\n",
        "        log_det_J = 0\n",
        "\n",
        "        for layer in self.layers:\n",
        "            log_det_J += layer.log_det_J(z)\n",
        "            z = layer(z)\n",
        "\n",
        "        return z, log_det_J"
      ],
      "metadata": {
        "id": "e9rSKzZt3OOZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_1= np.array([0,0]).astype('float32')\n",
        "mean_2=np.array([2,2.5]).astype('float32')\n",
        "mean_3=np.array([-3,10]).astype('float32')\n",
        "mean_4 = np.array([-5,1]).astype('float32')\n",
        "pdf1=MultivariateNormal(torch.tensor(mean_1), torch.eye(2))\n",
        "pdf2=MultivariateNormal(torch.tensor(mean_2), torch.eye(2))\n",
        "pdf3=MultivariateNormal(torch.tensor(mean_3), torch.eye(2))\n",
        "pdf4=MultivariateNormal(torch.tensor(mean_4), torch.eye(2))\n",
        "\n",
        "def multi_variate_gaussian_targ_distribution(x):\n",
        "    prob =4*torch.log(torch.tensor(.25,dtype=torch.float))+pdf1.log_prob(x)+pdf2.log_prob(x)+pdf3.log_prob(x)+pdf4.log_prob(x)\n",
        "    return torch.tensor(2)\n"
      ],
      "metadata": {
        "id": "kjH1Rdg14cyo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using a u shaped pdf\n",
        "import torch\n",
        "import torch.nn\n",
        "import matplotlib.pyplot as plt\n",
        "flow_length = 32\n",
        "dim = 2\n",
        "num_batches = 20000\n",
        "batch_size = 128\n",
        "lr = 6e-4\n",
        "model = PlanarFlow(dim, K=flow_length)\n",
        "target_distribution =U_1\n",
        "bound = VariationalLoss(target_distribution)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "for batch_num in range(1, num_batches + 1):\n",
        "        # Get batch from N(0,I).\n",
        "        batch = torch.zeros(size=(batch_size, 2)).normal_(mean=0, std=1)\n",
        "        # Pass batch through flow.\n",
        "        zk, log_jacobians = model(batch)\n",
        "        # Compute loss under target distribution.\n",
        "        loss = bound(batch, zk, log_jacobians)\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        if batch_num % 1000 == 0:\n",
        "            print(f\"(batch_num {batch_num:05d}/{num_batches}) loss: {loss}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaJJmxOe3Y2K",
        "outputId": "a2126249-ea24-4c6d-fab2-bd39f974e454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(batch_num 01000/20000) loss: 0.3203956186771393\n",
            "(batch_num 02000/20000) loss: -1.804951548576355\n",
            "(batch_num 03000/20000) loss: -1.8429417610168457\n",
            "(batch_num 04000/20000) loss: -1.6748672723770142\n",
            "(batch_num 05000/20000) loss: -1.779553771018982\n",
            "(batch_num 06000/20000) loss: -1.8872768878936768\n",
            "(batch_num 07000/20000) loss: -1.755571722984314\n",
            "(batch_num 08000/20000) loss: -1.8260223865509033\n",
            "(batch_num 09000/20000) loss: -1.8378047943115234\n",
            "(batch_num 10000/20000) loss: -1.8434269428253174\n",
            "(batch_num 11000/20000) loss: -1.8173214197158813\n",
            "(batch_num 12000/20000) loss: -1.7355355024337769\n",
            "(batch_num 13000/20000) loss: -1.8300026655197144\n",
            "(batch_num 14000/20000) loss: -1.8068687915802002\n",
            "(batch_num 15000/20000) loss: -1.887778401374817\n",
            "(batch_num 16000/20000) loss: -1.8225430250167847\n",
            "(batch_num 17000/20000) loss: -1.8791940212249756\n",
            "(batch_num 18000/20000) loss: -1.8250271081924438\n",
            "(batch_num 19000/20000) loss: -1.8884040117263794\n",
            "(batch_num 20000/20000) loss: -1.8479478359222412\n"
          ]
        }
      ]
    }
  ]
}